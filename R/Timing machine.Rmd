---
title: "Timing machine"
output: html_notebook
---

## Input generator
```{r setup, include=FALSE, res2ults='hide'}
library(igraph)
library(magrittr)
library(devtools)
setwd("~/Code/Warwick/BSP/R/")
document(pkg = "~/Code/Warwick/BSP/R/bspFunctions/") # compile the newest version of my package
library(bspFunctions)
```
Use a POETS graph to obtain a realistic distribution function:
```{r}
sourceG <- readPOETSGraph("./asp/Networks/nodes8000.edges")
dfit <- lnorm3_autofit(degree(sourceG))
```
Define an algorithm to be tested, for now just the default distance estimation from `igraph`:
```{r}
algoDistances <- function(g, params) {  # params will be passed by Timing Machine, ignored for now
  require(igraph)
  distances(g)
}
```

## Run series of sizes
Specyfying a range of input sizes we can obtain runtimes of the algorithm with a single function call:
```{r results='hide'}
sizes <- seq(200, to = 5000, by = 200)
res <- runOnSizes(algoDistances, graphFromDist,
sizes, 300,
inputArgs = dfit, algArgs = c())
saveRDS(res, file="from200to5000.rds")
# res2 <- readRDS(file="from200to5000.rds")
plot(res$nodes, res$elapsed)
```
Having gathered enough datapoints one can fit a linear model and asses visually how it matches the data:
```{r results="hide"}
squarefit <- lm(elapsed ~ poly(nodes, 2), data = res2)
# plot(squarefit)
{ plot(res2$nodes, res2$elapsed, col="red")
  points(res2$nodes, predict(squarefit, res2), col="green")}
```

And also inspect parameters of the model and various measures of it's uncertainity:
```{r}
summary(squarefit)
```
Note that in order to protect the model from correlation between powers of `nodes`, orthogonal polynomials are used. The `poly` function returns uncorrelated and normalised columns corressponding to consecutive powers of it's first argument.

As the model seems to fit our observations, we may try to extrapolate execution time of the same algorithm with a bigger input size:
```{r message=FALSE, results='hide'}
bigsizes <- c(6000, 8000, 10000, 12000)
bigres2 <- runSizes(algoDistances, graphFromDist,
                    bigsizes, 150,
                    inputArgs = dfit, algArgs = c())
```

```{r}
total <- rbind(res2, bigres2)
total$pred <- predict(squarefit, newdata = total)

matplot(total$nodes, total[c("pred", "elapsed")], col = c("green", "red"), pch = c(1, 19))
```
It comes out the prediction is a bit undershot but catches nonlinear behaviour of the data.

