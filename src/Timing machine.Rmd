---
title: "Timing machine"
output: html_notebook
---

## Input generator
Use a POETS graph to obtain a realistic distribution function:
```{r message=FALSE, warning=FALSE, include=FALSE}
library(here)
library(bspLib)
R.cache::setCacheRootPath(here("data", "cache"))
sourceG <- bspLib::readPOETSGraph(here( "external", "asp", "Networks", "nodes8000.edges"))
dfit <- lnorm3_autofit(degree(sourceG))
```
Define an algorithm to be tested, for now just the default distance estimation from `igraph`:
```{r}
algoDistances <- function(g, params) {  # params will be passed by Timing Machine, ignored for now
  require(igraph)
  distances(g)
}
```
Function call from C:
```{r}
library(Rcpp)
sourceCpp("bfs_from_R.cpp")
algoCDistances <- function(g, params) {
  # g must be an adjacency list here. No params taken
    Cbfs_list(g)
}
adjListFromDistr(size, distr){
  g <- graphFromDist(size, distr)
  g %>% as_adj_list %>% lapply(function(x) as.vector(x-1))
}
```

## Run series of sizes
Specyfying a range of input sizes we can obtain runtimes of the algorithm with a single function call:
```{r}
sizes <- seq(200, to = 5000, by = 200)
     # to be automated, use precomputed values if available
  res <- runOnSizes(algoDistances, graphFromDist,
                  sizes, 300,
                  inputArgs = dfit, algArgs = c())
```

```{r}
plot(res$nodes, res$elapsed)
```
Having gathered enough datapoints one can fit a linear model and asses visually how it matches the data:
```{r results="hide"}
squarefit <- lm(elapsed ~ poly(nodes, 2), data = res)
# plot(squarefit)
{ plot(res$nodes, res$elapsed, col="red")
  points(res$nodes, predict(squarefit, res), col="green")}
```

And also inspect parameters of the model and various measures of it's uncertainity:
```{r}
summary(squarefit)
```
Note that in order to protect the model from correlation between powers of `nodes`, orthogonal polynomials are used. The `poly` function returns uncorrelated and normalised columns corressponding to consecutive powers of it's first argument.

As the model seems to fit our observations, we may try to extrapolate execution time of the same algorithm with a bigger input size:
```{r message=FALSE, results='hide'}
bigsizes <- seq(6000, to=12000, by=2000)
bigres <- runOnSizes(algoDistances, graphFromDist,
                    bigsizes, 150,
                    inputArgs = dfit, algArgs = c())

```

```{r}
total <- rbind(res, bigres)
total$pred <- predict(squarefit, newdata = total)

matplot(total$nodes, total[c("pred", "elapsed")], col = c("green", "red"), pch = c(1, 19))
```
It comes out the prediction is a bit undershot but catches nonlinear behaviour of the data.

